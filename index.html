<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Safe Interpretability Workshop</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="">
    <!--link rel="canonical" href="xai-hcee.github.io/"-->

    <!-- Custom CSS -->
    <link rel="stylesheet" href="css/main.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
</head>


    <body>

    <header class="site-header">

  <div class="wrap">
    <a class="site-title" href="https://underline.io/events/458/sessions?eventSessionId=17411&searchGroup=lecture"><font color='#205caa'> The 1st Safe Interpretability Workshop</font> </a> 

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px"
           viewBox="0 0 18 15" enable-background="new 0 0 18 15" xml:space="preserve">
          <path fill="#505050" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0
            h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#505050" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484
            h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#505050" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0
            c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>
      <div class="trigger">
	  
      <a class="page-link" href="#overview">Overview</a>
        
      <a class="page-link" href="#speakers">Speakers</a>
      <a class="page-link" href="#panelists">Panelists</a>
      <a class="page-link" href="#cfp">Call for Papers</a>
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">
  <article class="post-content">
  <h2 id="overview">Overview</h2>

<ul>
  <li><strong>Date</strong>: TBD, 2025</li>
  <li><strong>Location</strong>: TBD</li>
  <!--li><strong>Schedule: TBD</strong>
    <ul>
      <li>9:00 - 9:40 Introduction, desiderata <a href="slides/section_1_slides.pdf">[slides]</a></li>
      <li>9:40 - 10:30 Prompting-based explanations <a href="slides/section_2_slides.pdf">[slides]</a></li>
      <li>10:30 - 11:00 Coffee break</li>
      <li>11:00 - 11:45 Data attribution <a href="slides/section_3_slides.pdf">[slides]</a></li>
      <li>11:45 - 12:30 Transformer Understanding <a href="slides/section_4_slides.pdf">[slides]</a></li>
      <li>12:30 - 12:45 Conclusion and Q&amp;A <a href="slides/section_5_slides.pdf">[slides]</a></li>
      <li><a href="https://us06web.zoom.us/rec/play/RRdA6IuIX13g6xLLjgUgcMQmXGHEe3u6uisMPsDO8_WNEmZHou3USuTXmxeKnjUdHmJendcsJpfFiJD3.FL1kpT1LOtq-dQLU?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fus06web.zoom.us%2Frec%2Fshare%2F4145_KgnrsRMq0nodtjU6qXkiPK5slep8gEWUKRBWBjlkGvapTNB5lsE7m7CGKYj.IycRR_E0KeSoB9vL">Zoom recording</a> for the tutorial</li>
    </ul>
  </li-->
  <!--li><strong>Underline page</strong>: <a href="https://2024.naacl.org/program/tutorials/">Explanations in the Era of Large Language Models</a></li-->
</ul>

<p>As the DNN-based AI systems demonstrate unprecedented capabilities, they are deployed widely and have far-reaching impacts, leading to an increasing concern about their safety. How will they be detrimental to the users, the internet, and the society as a whole? What are the driving reasons behind these adverse effects? Will their strong capabilities be utilized to build up attacks, and in what ways can we defend against these attacks? Currently, these questions are studied in multiple research fields, including but not limited to interpretability  and cybersecurity. This workshop aims at bringing together multiple communities, foster novel collaboration opportunities and sparkle new ideas towards solving these critical threats.</p>


<!--p>Click on each chapter to view the video recording and the slides:</p>
<ul>
  <li><a href="https://xai-hcee.github.io/chapter_0.html">Chapter 0: Introduction</a></li>
  <li><a href="https://xai-hcee.github.io/chapter_1.html">Chapter 1: Psychological foundations of explanations</a></li>
  <li><a href="https://xai-hcee.github.io/chapter_2.html">Chapter 2: Overview of XAI techniques</a></li>
  <li><a href="https://xai-hcee.github.io/chapter_3.html">Chapter 3: Application-grounded human-subject evaluations</a></li>
  <li><a href="https://xai-hcee.github.io/chapter_4.html">Chapter 4: Proxy evaluations through human-provided explanations</a></li>
  <li><a href="https://xai-hcee.github.io/chapter_5.html">Chapter 5: Summary and future directions</a></li>
</ul-->

<h2 id="speakers">Speakers</h2>
<div class="col-md-12 profile">
  <div class="height150 col-md-3">
    <div><a href="https://yoshuabengio.org/"><img class="avatar-img" width="150" src="images/yoshua_bengio.png" /> </a></div>
    <div><center><b>Yoshua Bengio</b><br />Université de Montréal</center></div>
  </div>
  <div class="col-md-9">
    Recognized worldwide as one of the leading experts in artificial intelligence, Yoshua Bengio is most known for his pioneering work in deep learning, earning him the 2018 A.M. Turing Award with Geoffrey Hinton and Yann LeCun. He is Full Professor at Université de Montréal, and Founder and Scientific Advisor of Mila – Quebec AI Institute. Concerned about the social impact of AI, he actively contributed to the Montreal Declaration for the Responsible Development of Artificial Intelligence and currently chairs the International Scientific Report on the Safety of Advanced AI.
  </div>
</div>
<div class="col-md-12 profile">
  <div class="height150 col-md-3">
    <div><a href="https://web.cs.dal.ca/~rudzicz/"><img class="avatar-img" width="150" src="images/frank_rudzicz.jpeg" /> </a></div>
    <div><center><b>Frank Rudzicz</b><br />Dalhousie University</center></div>
  </div>
  <div class="col-md-9">
    Frank Rudzicz is an Associate Professor in the Faculty of Computer Science at Dalhousie University. His research is in machine learning in healthcare, especially in natural language processing, speech, and safety. Frank is also the co-founder of several health+AI companies including WinterLight Labs. He received the NAACL Outstanding paper award, and his research appeared in the New York Times, the Globe &amp; Mail, Wired, Maclean’s, Space, CTV News, CBC News, the Toronto Star, Nature, and Scientific American.
  </div>
</div>
<div class="col-md-12 profile">
  <div class="height150 col-md-3">
    <div><a href="https://www.xiaojingliao.com/"><img class="avatar-img" width="150" src="images/xiaojing_liao.jpg" /> </a></div>
    <div><center><b>Xiaojing Liao</b><br />Indiana University Bloomington</center></div>
  </div>
  <div class="col-md-9">
    Xiaojing Liao is an Assistant Professor in the Department of Computer Science at Indiana University Bloomington. Her research interests are to discover and understand critical security issues in a large system through data-oriented security analysis, and then to design and develop innovative solutions to address these issues. She has received the Meta Privacy-Enhancing Technology Research Award, and multiple paper awards from ACM CCS, ISOC NDSS, and SIGSAC Doctoral Dissertation.
  </div>
</div>
<div class="col-md-12 profile">
  <div class="height150 col-md-3">
    <div><a href="https://ziyuyao.org/"><img class="avatar-img" width="150" src="images/ziyu.jpg" /> </a></div>
    <div><center><b>Ziyu Yao</b><br />George Mason University</center></div>
  </div>
  <div class="col-md-9">
    Ziyu Yao is an Assistant Professor in the Department of Computer Science at George Mason University. Her research involves knowledge grounding, reasoning, and planning, responsible and trustworthy natural language inferences, and interdisciplinary applications. Her research has been funded by National Science Foundation, Microsoft Accelerate Foundation Models Research Award, Virginia Commonwealth Cyber Initiative, and UMD’s Applied Research Laboratory for Intelligence and Security. She was the Diversity & Inclusion Co-Chair at NAACL 2024 and the lead faculty organizer of MASC-SLL 2023. She co-organized SUKI at NAACL 2022 and NLP4Prog at ACL 2021. 
  </div>
</div>

<h2 id="panelists">Panelists (tentative)</h2>
<div class="row">
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://shubhobm.github.io/"><img class="avatar-img" width="150" src="images/subho.jpeg" /> </a></div>
        <div style="margin-bottom:40px"><center><b>Subho Majumdar</b><br />Vijil, ARVA</center></div>
      </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://jeknov.github.io/"><img class="avatar-img" width="150" src="images/jekaterina.jpg" /> </a></div>
        <div style="margin-bottom:40px"><center><b>Jekaterina Novikova</b><br />ARVA, SIGSEC</center></div>
      </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://william-eiers.github.io/"><img class="avatar-img" width="150" src="images/william_eiers.jpg" /> </a></div>
        <div style="margin-bottom:40px"><center><b>William Eiers</b><br />SIT</center></div>
      </div>
</div>
</div>

<h2 id="organizers">Organizers</h2>
<div class="row">
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="http://ecailabs.org"><img class="avatar-img" width="150" src="images/zining.png" /> </a></div>
        <div style="margin-bottom:40px"><center><b>Zining Zhu</b><br />SIT</center></div>
      </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://hanjiechen.github.io/"><img class="avatar-img" width="150" src="images/hanjie.jpeg" /> </a></div>
        <div style="margin-bottom:40px"><center><b>Hanjie Chen</b><br />Rice</center></div>
    </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://www.ruixiangtang.net/"><img class="avatar-img" width="150" src="images/ruixiang.png" /></a></div>
        <div style="margin-bottom:40px"><center><b>Ruixiang Tang</b><br />Rutgers</center></div>
    </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://www.linkedin.com/in/itay-yona-b40a7756/"><img class="avatar-img" width="150" src="images/itay.jpeg" /></a></div>
        <div style="margin-bottom:40px"><center><b>Itay Yona</b><br />DeepMind</center></div>
    </div>
</div>
</div>

<h2 id="cfp">Call for Papers</h2>
<h3>Topics of Interest</h3>
<p>We invite papers in all areas related to the safety and the transparency of AIs, including but not limited to:
<ul>
  <li>Interpretability, explainability and trustworthiness of AI</li>
  <li>Adversarial attacks and defenses</li>
  <li>Robustness, reliability and verifiability of AI systems</li>
  <li>Applications of AI in safety-critical domains</li>
  <li>AI for safety, security and privacy</li>
  <li>AI for cybersecurity</li>
</ul>
</p>
<h3>Submission Guidelines</h2>
<p>All papers should follow the NeurIPS template, with an 8-page limitation, excluding references. All submissions are double-blind. Arxiv or other preprints are allowed. Accepted papers will be presented as posters or talks, with the option to be non-archival.</p>
<p><b>Submission link:</b> OpenReview</p> 

<h3>Important Dates</h3>
<ul>
  <li>Submission deadline: TBD</li>
  <li>Authors notification: TBD</li>
  <li>Camera-ready deadline: TBD</li>
</ul>


<!--div class="col-md-12">
    <h2>Recommended Reading</h2>
    <ul>
        <li><a href="https://projects.illc.uva.nl/indeep/tutorial/">EACL 24 Tutorial: Transformer-specific Interpretability</a></li>
        <li><a href="https://ziningzhu.notion.site/Explanations-reading-list-56cd203b1d1c4fd79e8fcf319b1560a8">Explanations reading list</a></li>
        <li><a href="https://xai-hcee.github.io">NAACL 22 Tutorial on Human-centered Evaluations of Explanations</a></li>
        <li><a href="https://aclanthology.org/2020.emnlp-tutorials.3/">EMNLP 20 Tutorial: Interpreting Predictions of NLP Models</a></li>
        <li><a href="https://aclanthology.org/2020.acl-tutorials.1/">ACL 20 Tutorial: Interpretability and Analysis in Neural NLP</a></li>
    </ul>
</div-->
  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <h2 class="footer-heading">Safe interpretability workshop 2025</h2>

    <div class="footer-col-3 column">
      <ul>
        <li>The stylesheets come from <a href="https://explanation-llm.github.io">explanation-llm</a>.</li>
        <li> For questions related to the website please email the organizers.</li>
      </ul>
    </div>



  </div>

</footer>


    </body>
</html>